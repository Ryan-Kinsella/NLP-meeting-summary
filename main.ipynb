{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Google Cloud Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\R-k-l\\Documents\\AA Fouth Year\\BetaLab\\astute-citadel-268715-f42d2b0c92bb.JSON\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'C:\\Users\\R-k-l\\Documents\\AA Fouth Year\\BetaLab\\astute-citadel-268715-f42d2b0c92bb.JSON'\n",
    "client = speech.SpeechClient()\n",
    "print(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: But what if somebody decides to break it be careful that you keep adequate coverage, but look for places to save money baby. It's taking longer to get things squared away. Then the bankers expected hiring the life for one's company may win her tax aided retirement income the booth just helpful, but inadequate new self to saving Rags or hardly tossed on the two naked bones. What it discussion Canyon Sue when the title of this type of song is in question. There's no dying or waxing or gassing need a paperweight maybe personalized known back Quoc Leigh is leather hard place work on a flat surface and smooth out this simplest kind of separate system uses a single self-contained unit to the old shop outage still Holts a good mechanic is usually a bad boss. So fingers would go higher in later years. So make beautiful chairs cabinets chest doll houses. It's at\n"
     ]
    }
   ],
   "source": [
    "file_name = r'C:\\Users\\R-k-l\\Documents\\AA Fouth Year\\BetaLab\\male.WAV'\n",
    "with io.open(file_name, 'rb') as audio_file:\n",
    "    content = audio_file.read()\n",
    "    audio = types.RecognitionAudio(content=content)\n",
    "    \n",
    "config = types.RecognitionConfig(\n",
    "    encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "    sample_rate_hertz=8000,\n",
    "    language_code='en-US')\n",
    "\n",
    "response = client.recognize(config, audio)\n",
    "for result in response.results:\n",
    "    print('Transcript: {}'.format(result.alternatives[0].transcript))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing\n",
    "Tutorial used,\n",
    "https://medium.com/biaslyai/beginners-guide-to-text-preprocessing-in-python-2cbeafbf5f44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\R-k-l\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\R-k-l\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re                      # Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_s(sentences): # same can be achieved for words tokens\n",
    "    s_new = []\n",
    "    for sent in (sentences[:][0]): #For NumpY = sentences[:]\n",
    "        s_token = sent_tokenize(sent)\n",
    "        if s_token != '':\n",
    "            s_new.append(s_token)\n",
    "    return s_new\n",
    "\n",
    "def tokenization_w(words):\n",
    "    w_new = []\n",
    "    for w in (words[:][0]):  # for NumPy = words[:]\n",
    "        w_token = word_tokenize(w)\n",
    "        if w_token != '':\n",
    "            w_new.append(w_token)\n",
    "    return w_new\n",
    "\n",
    "def preprocess(text):\n",
    "    clean_data = []\n",
    "    for x in (text[:][0]): #this is Df_pd for Df_np (text[:])\n",
    "        new_text = re.sub('<.*?>', '', x)   # remove HTML tags\n",
    "        new_text = re.sub(r'[^\\w\\s]', '', new_text) # remove punc.\n",
    "        new_text = re.sub(r'\\d+','',new_text)# remove numbers\n",
    "        new_text = new_text.lower() # lower case, .upper() for upper          \n",
    "        if new_text != '':\n",
    "            clean_data.append(new_text)\n",
    "    return clean_data\n",
    "\n",
    "def stemming(words):\n",
    "    snowball = SnowballStemmer(language = 'english')\n",
    "    new = []\n",
    "    stem_words = [snowball.stem(x) for x in (words[:][:])]\n",
    "    new.append(stem_words)\n",
    "    return new\n",
    "\n",
    "def lemmatization(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new = []\n",
    "    lem_words = [lemmatizer.lemmatize(x) for x in (words[:][:])]\n",
    "    new.append(lem_words)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = 'This is going to be one of the biggest concerts ever! I am super excited, \\\n",
    "I brought my favourite pair of jorts to rage in. I do not want to look like a basic party concert \\\n",
    "dude, but if it happens then it is all good. Actually never mind, I went to the concert and did not expect everyone to be wearing snowsuits, I felt \\\n",
    "a little bit out of place wearing my embarrassing jorts.'\n",
    "paragraph =[]\n",
    "paragraph.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is going to be one of the biggest concert...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  This is going to be one of the biggest concert..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paragraph = pd.DataFrame(paragraph)\n",
    "df_paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This is going to be one of the biggest concerts ever!',\n",
       "  'I am super excited, I brought my favourite pair of jorts to rage in.',\n",
       "  'I do not want to look like a basic party concert dude, but if it happens then it is all good.',\n",
       "  'Actually never mind, I went to the concert and did not expect everyone to be wearing snowsuits, I felt a little bit out of place wearing my embarrassing jorts.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_sentences = tokenization_s(df_paragraph)\n",
    "token_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is going to be one of the biggest concerts ever',\n",
       " 'i am super excited i brought my favourite pair of jorts to rage in',\n",
       " 'i do not want to look like a basic party concert dude but if it happens then it is all good',\n",
       " 'actually never mind i went to the concert and did not expect everyone to be wearing snowsuits i felt a little bit out of place wearing my embarrassing jorts']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_paragraph = preprocess(token_sentences)\n",
    "preproc_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_paragraph = stemming(preproc_paragraph)\n",
    "stem_paragraph[0][60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_paragraph = lemmatization(preproc_paragraph)\n",
    "lem_paragraph[0][60:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'biggest',\n",
       " 'concerts',\n",
       " 'ever',\n",
       " '!',\n",
       " 'I',\n",
       " 'am',\n",
       " 'super']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_words = tokenization_w(df_paragraph)\n",
    "token_words[0][0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'biggest',\n",
       " 'concerts',\n",
       " 'ever',\n",
       " 'i',\n",
       " 'am',\n",
       " 'super',\n",
       " 'excited']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_words = preprocess(token_words)\n",
    "preproc_words[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'go',\n",
       " 'to',\n",
       " 'be',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'biggest',\n",
       " 'concert',\n",
       " 'ever',\n",
       " 'i',\n",
       " 'am',\n",
       " 'super',\n",
       " 'excit']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words = stemming(preproc_words)\n",
    "stem_words[0][0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'biggest',\n",
       " 'concert',\n",
       " 'ever',\n",
       " 'i',\n",
       " 'am',\n",
       " 'super',\n",
       " 'excited']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemit = lemmatization(preproc_words)\n",
    "lemit[0][0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read a sample .txt file, and perform preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Shepherd Boy tended his master\\'s Sheep near a dark forest not far from the village. Soon he found life in the pasture very dull. All he could do to amuse himself was to talk to his dog or play on his shepherd\\'s pipe. One day as he sat watching the Sheep and the quiet forest, and thinking what he would do should he see a Wolf, he thought of a plan to amuse himself. His Master had told him to call for help should a Wolf attack the flock, and the Villagers would drive it away. So now, though he had not seen anything that even looked like a Wolf, he ran toward the village shouting at the top of his voice, \"Wolf! Wolf!\" As he expected, the Villagers who heard the cry dropped their work and ran in great excitement to the pasture. But when they got there they found the Boy doubled up with laughter at the trick he had played on them. A few days later the Shepherd Boy again shouted, \"Wolf! Wolf!\" Again the Villagers ran to help him, only to be laughed at again. Then one evening as the sun was setting behind the forest and the shadows were creeping out over the pasture, a Wolf really did spring from the underbrush and fall upon the Sheep. In terror the Boy ran toward the village shouting \"Wolf! Wolf!\" But though the Villagers heard the cry, they did not run to help him as they had before. \"He cannot fool us again,\" they said. The Wolf killed a great many of the Boy\\'s sheep and then slipped away into the forest.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('the_boy_who_cried_wolf.txt', 'r') as file:\n",
    "    text = file.read().replace('\\n', '')\n",
    "    file.close()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the following tasks on the data:\n",
    "- Convert everything to lowercase\n",
    "- Remove HTML tags\n",
    "- Contraction mapping\n",
    "- Remove (â€˜s)\n",
    "- Remove any text inside the parenthesis ( )\n",
    "- Eliminate punctuations and special characters\n",
    "- Remove stopwords\n",
    "- Remove short words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
