{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from attention import AttentionLayer\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "from bs4 import BeautifulSoup \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords   \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"train_model_files/article_summary_and_article_title/sample_1000.csv\")\n",
    "# data.drop_duplicates(subset=['article'],inplace=True)#dropping duplicates\n",
    "data=pd.read_csv(\"../Reviews.csv\",nrows=100000)\n",
    "data.drop_duplicates(subset=['Text'],inplace=True)  #dropping duplicates\n",
    "data.dropna(axis=0,inplace=True)#dropping na rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary  \\\n",
       "0  Good Quality Dog Food   \n",
       "1      Not as Advertised   \n",
       "2  \"Delight\" says it all   \n",
       "3         Cough Medicine   \n",
       "4            Great taffy   \n",
       "\n",
       "                                                                                                                                                                                                      Text  \n",
       "0  I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labr...  \n",
       "1           Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".  \n",
       "2  This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with ...  \n",
       "3  If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The fl...  \n",
       "4                                                             Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.rename(columns = {'article': 'Text', 'title': 'Summary'})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString, exclude_encodings=\"lxml\").text # removes html/xml taggs\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                  #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "cleaned_text = []\n",
    "for t in data['Text']:\n",
    "    cleaned_text.append(text_cleaner(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_cleaner(text):\n",
    "    newString = re.sub('\"','', text)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    newString = newString.lower()\n",
    "    tokens=newString.split()\n",
    "    newString=''\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                 \n",
    "            newString=newString+i+' '  \n",
    "    return newString\n",
    "\n",
    "#Call the above function\n",
    "cleaned_summary = []\n",
    "for t in data['Summary']:\n",
    "    cleaned_summary.append(summary_cleaner(t))\n",
    "\n",
    "data['cleaned_text']=cleaned_text\n",
    "data['cleaned_summary']=cleaned_summary\n",
    "data['cleaned_summary'].replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better\n",
      "Summary: good quality dog food \n",
      "\n",
      "Review: product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo\n",
      "Summary: not as advertised \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(\"Review:\",data['cleaned_text'][i])\n",
    "    print(\"Summary:\",data['cleaned_summary'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in data['cleaned_text']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['cleaned_summary']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_text=27\n",
    "max_len_summary=15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply start of sentence/ end of sentence tokens to summary, used for \n",
    "data['cleaned_summary'] = data['cleaned_summary'].apply(lambda x : 'sostok '+ x + ' eostok') # start of sentence token, end of sentence token\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(data['cleaned_text'],data['cleaned_summary'],test_size=0.1,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post') \n",
    "x_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
    "\n",
    "x_voc_size   =  len(x_tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#preparing a tokenizer for summary on training data \n",
    "y_tokenizer = Tokenizer()\n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert summary sequences into integer sequences\n",
    "y_tr    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr, maxlen=max_len_summary, padding='post')\n",
    "y_val   =   pad_sequences(y_val, maxlen=max_len_summary, padding='post')\n",
    "\n",
    "y_voc_size  =   len(y_tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_tr)):\n",
    "    cnt=0\n",
    "    for j in y_tr[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr=np.delete(y_tr,ind, axis=0)\n",
    "x_tr=np.delete(x_tr,ind, axis=0)\n",
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79516, 79516)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenizer.word_counts['sostok'],len(y_tr) # should be the same size, check if there are any sostok's in the word list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Creating embeddings_index took 11.5 seconds.\n"
     ]
    }
   ],
   "source": [
    "from load_glove_embeddings import load_glove_embeddings # local function from .py file\n",
    "glove_dimension = 50\n",
    "import time\n",
    "timer_start = time.time()\n",
    "word2index, embedding_matrix = load_glove_embeddings('../glove.6B.'+str(glove_dimension)+'d.txt', embedding_dim=glove_dimension, include_empty_char=False)\n",
    "print('Found %s word vectors.' % len(word2index))\n",
    "print(\"Creating embeddings_index took\", round(time.time() - timer_start, 1), \"seconds.\")\n",
    "del timer_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41800001,  0.24968   , -0.41242   , ..., -0.18411   ,\n",
       "        -0.11514   , -0.78580999],\n",
       "       [ 0.013441  ,  0.23682   , -0.16899   , ..., -0.56656998,\n",
       "         0.044691  ,  0.30392   ],\n",
       "       [ 0.15164   ,  0.30177   , -0.16763   , ..., -0.35652   ,\n",
       "         0.016413  ,  0.10216   ],\n",
       "       ...,\n",
       "       [-0.51181   ,  0.058706  ,  1.09130001, ..., -0.25003001,\n",
       "        -1.125     ,  1.58630002],\n",
       "       [-0.75897998, -0.47426   ,  0.47369999, ...,  0.78953999,\n",
       "        -0.014116  ,  0.64480001],\n",
       "       [ 0.072617  , -0.51393002,  0.47279999, ..., -0.18907   ,\n",
       "        -0.59021002,  0.55558997]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0),\n",
       " (',', 1),\n",
       " ('.', 2),\n",
       " ('of', 3),\n",
       " ('to', 4),\n",
       " ('and', 5),\n",
       " ('in', 6),\n",
       " ('a', 7),\n",
       " ('\"', 8),\n",
       " (\"'s\", 9)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "word2index\n",
    "list(islice(word2index.items(), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_matrix_creater(embedding_dimention, word_index): \n",
    "    \"\"\"\n",
    "    Uses GloVe as a global word embedding. \n",
    "    \n",
    "    embedding_dimention: usually in the title of glove.6D.'embedding_dimention', in this notebook use 50. \n",
    "    word_index: the input word embeddings\n",
    "    \n",
    "    returns: a local embedding matrix, to be input as weights [embedding] for the constructor\n",
    "        for keras' Embedding object. \n",
    "    \"\"\"\n",
    "    local_embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimention))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = word2index.get(word) # from cell above, from glove.\n",
    "        if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all-zeros.\n",
    "            local_embedding_matrix[i] = embedding_vector\n",
    "    return local_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51571, 50)\n",
      "(14098, 50)\n"
     ]
    }
   ],
   "source": [
    "text_embedding_matrix = embedding_matrix_creater(glove_dimension, word_index=x_tokenizer.word_index) \n",
    "print(text_embedding_matrix.shape)\n",
    "\n",
    "sum_embedding_matrix = embedding_matrix_creater(glove_dimension, word_index=y_tokenizer.word_index)\n",
    "print(sum_embedding_matrix.shape)\n",
    "\n",
    "encoder_embedding_layer = Embedding(input_dim = int(text_embedding_matrix.shape[0]), # vocab size\n",
    "                                    output_dim = int(text_embedding_matrix.shape[1]), # embedding dimension\n",
    "                                    input_length = max_len_text,\n",
    "                                    weights = [text_embedding_matrix],\n",
    "                                    trainable = True) # changed from false\n",
    "\n",
    "decoder_embedding_layer = Embedding(input_dim = int(sum_embedding_matrix.shape[0]), \n",
    "                                    output_dim = int(sum_embedding_matrix.shape[1]),\n",
    "                                    input_length = max_len_summary,\n",
    "                                    weights = [sum_embedding_matrix],\n",
    "                                    trainable = True) # changed from false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_dim = 500 \n",
    "\n",
    "# # Encoder \n",
    "# encoder_inputs = Input(shape=(max_len_text,)) \n",
    "# enc_emb = Embedding(x_voc_size, latent_dim,trainable=True)(encoder_inputs) \n",
    "\n",
    "# #LSTM 1 \n",
    "# encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "# encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "\n",
    "# #LSTM 2 \n",
    "# encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "# encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "\n",
    "# #LSTM 3 \n",
    "# encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "# encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
    "\n",
    "# # Set up the decoder. \n",
    "# decoder_inputs = Input(shape=(None,)) \n",
    "# dec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True) \n",
    "# dec_emb = dec_emb_layer(decoder_inputs) \n",
    "\n",
    "# #LSTM using encoder_states as initial state\n",
    "# decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "# decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
    "\n",
    "# #Attention Layer\n",
    "# attn_layer = AttentionLayer(name='attention_layer') \n",
    "# attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "# # Concat attention output and decoder LSTM output \n",
    "# decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# #Dense layer\n",
    "# decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax')) \n",
    "# decoder_outputs = decoder_dense(decoder_concat_input) \n",
    "\n",
    "# # Define the model\n",
    "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 27)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 27, 50)       2578550     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 27, 300), (N 421200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 27, 300), (N 721200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 50)     704900      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 27, 300), (N 721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 300),  421200      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 300),  180300      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 600)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 14098)  8472898     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 14,221,448\n",
      "Trainable params: 14,221,448\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# (number of inputs, length of the output sentence, the number of words in the output)\n",
    "from keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "latent_dim = 300\n",
    "# embedding_dim=100 # OLD\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_len_text,))\n",
    "\n",
    "#embedding layer\n",
    "# enc_emb =  Embedding(x_voc_size, embedding_dim,trainable=True)(encoder_inputs) # OLD\n",
    "enc_emb = encoder_embedding_layer(encoder_inputs) # NEW\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,)) \n",
    "# decoder_inputs = Input(shape=(max_len_text,))\n",
    "\n",
    "#embedding layer\n",
    "# dec_emb_layer = Embedding(y_voc_size, embedding_dim,trainable=True) # OLD\n",
    "# dec_emb = dec_emb_layer(decoder_inputs) # OLD\n",
    "dec_emb = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc_size, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (number of inputs, length of the output sentence, the number of words in the output)\n",
    "# from keras import backend as K \n",
    "# K.clear_session()\n",
    "\n",
    "\n",
    "# latent_dim = 300\n",
    "# embedding_dim=100\n",
    "\n",
    "# # Encoder\n",
    "# encoder_inputs = Input(shape=(max_len_text,))\n",
    "\n",
    "# #embedding layer\n",
    "# enc_emb =  Embedding(x_voc_size, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "# #encoder lstm 1\n",
    "# encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "# encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# #encoder lstm 2\n",
    "# encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "# encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# #encoder lstm 3\n",
    "# encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "# encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# # Set up the decoder, using `encoder_states` as initial state.\n",
    "# decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# #embedding layer\n",
    "# dec_emb_layer = Embedding(y_voc_size, embedding_dim,trainable=True)\n",
    "# dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "# decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# # Attention layer\n",
    "# attn_layer = AttentionLayer(name='attention_layer')\n",
    "# attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# # Concat attention input and decoder LSTM output\n",
    "# decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# #dense layer\n",
    "# decoder_dense =  TimeDistributed(Dense(y_voc_size, activation='softmax'))\n",
    "# decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# # Define the model \n",
    "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79516 samples, validate on 8768 samples\n",
      "Epoch 1/2\n",
      "79516/79516 [==============================] - 3000s 38ms/sample - loss: 2.0687 - val_loss: 1.9697\n",
      "Epoch 2/2\n",
      "79516/79516 [==============================] - 2764s 35ms/sample - loss: 2.0442 - val_loss: 1.9690\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([x_tr,y_tr[:,:-1]], \n",
    "                  y_tr.reshape(y_tr.shape[0],\n",
    "                  y_tr.shape[1], 1)[:,1:],\n",
    "                  epochs=2,\n",
    "                  callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1)],\n",
    "                  batch_size=32,\n",
    "                  validation_data=([x_val,y_val[:,:-1]],y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot \n",
    "pyplot.plot(history.history['loss'], label='train') \n",
    "pyplot.plot(history.history['val_loss'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferernce, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word \n",
    "reverse_source_word_index=x_tokenizer.index_word \n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_len_text,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= decoder_embedding_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "#     print('e_out:', e_out)\n",
    "#     print('e_h:', e_h)\n",
    "    # works fine\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Chose the 'start' word as the first word of the target sequence\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "    # works fine\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "#         print(\"output_tokens:\", output_tokens)\n",
    "        # output tokens are usually 3\n",
    "    \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "#         print(sampled_token_index)\n",
    "#         if sampled_token_index != 0: # loops here too many times, shouldn't have an index of 0, but consistently does. \n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        print('sample token:', sampled_token)\n",
    "        \n",
    "        if(sampled_token!='eostok' and sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok' or len(decoded_sentence.split()) >= (max_len_summary-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        \n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: ordered salmon thursday january received january salmon delicious wooden box nice design used store items future \n",
      "Original summary: alaska smokehouse smoked salmon \n",
      "\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: sweetened coffee beverages fridge work noticed people stopped consuming happened around time lost interest level sweetness recommend drink prefer lower level sugar old school arabica coffee types \n",
      "Original summary: thought it was perfect little sweet without being too sweet \n",
      "\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: variety granola one ranks top favorites crunch granola coconut main taste sweet overly eat snack craving something fills fast satisfies sugar cravings \n",
      "Original summary: great taste \n",
      "\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: bought dollar tree yummy taste like french fries crunchy going buy bulk pass kid test \n",
      "Original summary: yum \n",
      "\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: used eating flaxseed brownie hodgson mill brownies super easy make taste great since like dark chocolate usually add little cocoa \n",
      "Original summary: delicious brownie \n",
      "\n",
      "sample token: great\n",
      "sample token: eostok\n",
      "Predicted summary:  great\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(\"Review:\",seq2text(x_val[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_val[i]))\n",
    "    print()\n",
    "    print(\"Predicted summary:\",decode_sequence(x_val[i].reshape(1,max_len_text)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101,  821, 7479, 3722,  209, 3722,  821,   63, 5388,   47,   61,\n",
       "        1537,   43,   55,  601,  767,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[0].reshape(1,max_len_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 266,  411,    4, 5620,  248,  854,  599,  411,  206,  953,    3,\n",
       "          42,  800,   33,  102, 1002,   69, 2510,  318, 2789,   36, 2011,\n",
       "           0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[2].reshape(1,max_len_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 266,  411,    4, 5620,  248,  854,  599,  411,  206,  953,    3,\n",
       "         42,  800,   33,  102, 1002,   69, 2510,  318, 2789,   36, 2011,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def doubleData(x):\n",
    "#     print(str(x))\n",
    "#     if str(x).startswith('start'): \n",
    "#         print(str(x))\n",
    "#         temp=str(x)\n",
    "        \n",
    "#         return True\n",
    "# modDfObj = data.apply(doubleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 13 14  2  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[  1  34 168  46   2   0   0   0   0   0   0   0   0   0   0]\n",
      "[   1 4595    2    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n",
      "[   1  120  251 2481   36  579 2317   12   50  579   89    2    0    0\n",
      "    0]\n",
      "[   1    7   37  913   62   68  887 2665    2    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(y_tr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n"
     ]
    }
   ],
   "source": [
    "# Save model.\n",
    "model_json = model.to_json()\n",
    "with open('model_v1.json',\"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('model_v1.h5')\n",
    "print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown initializer: GlorotUniform",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-4a4cba2c45fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloaded_model_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_model_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# load weights into new model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model_v1.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\r-k-l\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mmodel_from_json\u001b[1;34m(json_string, custom_objects)\u001b[0m\n\u001b[0;32m    662\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\r-k-l\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    166\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32mc:\\users\\r-k-l\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    145\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[1;32m--> 147\u001b[1;33m                                         list(custom_objects.items())))\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\r-k-l\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m   1054\u001b[0m         \u001b[1;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layers'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m             \u001b[0mprocess_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m         \u001b[1;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\r-k-l\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[1;34m(layer_data)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             layer = deserialize_layer(layer_data,\n\u001b[1;32m-> 1042\u001b[1;33m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[0mcreated_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\r-k-l\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    166\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32mc:\\users\\r-k-l\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    147\u001b[0m                                         list(custom_objects.items())))\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[1;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\r-k-l\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m   2349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'implementation'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'implementation'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2350\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'implementation'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2351\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\r-k-l\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\r-k-l\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, implementation, return_sequences, return_state, go_backwards, stateful, unroll, **kwargs)\u001b[0m\n\u001b[0;32m   2229\u001b[0m                         \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2230\u001b[0m                         \u001b[0mrecurrent_dropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrecurrent_dropout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2231\u001b[1;33m                         implementation=implementation)\n\u001b[0m\u001b[0;32m   2232\u001b[0m         super(LSTM, self).__init__(cell,\n\u001b[0;32m   2233\u001b[0m                                    \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\r-k-l\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, implementation, **kwargs)\u001b[0m\n\u001b[0;32m   1881\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecurrent_initializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_initializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\r-k-l\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\initializers.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class_name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'config'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\r-k-l\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\initializers.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    508\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                                     printable_module_name='initializer')\n\u001b[0m\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\r-k-l\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[1;32m--> 140\u001b[1;33m                                  ': ' + class_name)\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'from_config'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mcustom_objects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown initializer: GlorotUniform"
     ]
    }
   ],
   "source": [
    "# Note: may be unable to load model since a custom \"attention layer\" is used. \n",
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('model_v1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_v1.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
